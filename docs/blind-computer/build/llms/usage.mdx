import Tabs from '@theme/Tabs';
import TabItem from '@theme/TabItem';

# Usage

Once you have [nilAI API access](https://nilai.nillion.com), you can start using LLMs on nilAI with any OpenAI-compatible library.

## Getting Started with Private LLMs

<Tabs>

    <TabItem value="python" label="Python">
        <Tabs>
            <TabItem value="pip" label="pip" default>
                ```bash
                pip install nilai-py
                ```
            </TabItem>
            <TabItem value="uv" label="uv">
                ```bash
                uv pip install nilai-py
                ```
            </TabItem>
        </Tabs>
    </TabItem>
    <TabItem value="typescript" label="TypeScript" default>
        ```bash
        pnpm install @nillion/nilai-ts
        ```
    </TabItem>

</Tabs>

You can either use:

- `API Key` flow as the sole developer / organization or
- `Direct Signing.` to provide permissions to another user / organization.

### API Key Flow

1. Use `https://api.nilai.nillion.network/v1` as the BASE URL
2. Check [available models](/blind-computer/build/llms/overview#available-models) or query the [`/v1/models`](/api/nilai/get-models-v-1-models-get) endpoint or
3. Select an available model and use it with the [`/v1/chat/completions`](/api/nilai/chat-completion-v-1-chat-completions-post) nilAI node endpoint

With OpenAI compatibility, you can use any OpenAI library. Here's an example for querying the `gpt-oss-20b` model:

<Tabs>
    <TabItem value="python" label="Python">
        ```python reference showGithubLink
        https://github.com/NillionNetwork/nilai-py/blob/main/examples/0-api_key_mode.py
        ```
    </TabItem>
    <TabItem value="typescript" label="Typescript" default>
        ```typescript reference showGithubLink
        https://github.com/NillionNetwork/nilai-ts/blob/main/examples/0-api-key.ts
        ```
    </TabItem>

</Tabs>

### Direct Signing.

To use the direct signing flow, you need to create a delegation token server.

The server creates delegation tokens and manages their expiration and usage. Then the delegation token allows clients to make requests to the nilAI API.

<Tabs>
    <TabItem value="nextjs-nucs" label="Next.js" default>
        This flow matches the older `secretllm_nextjs_nucs` tutorial:

        1. Server initializes `DelegationTokenServer` with `expirationTime` and `tokenMaxUses`.
        2. Client initializes `NilaiOpenAIClient` with `authType: AuthType.DELEGATION_TOKEN`.
        3. Client produces a delegation request with `client.getDelegationRequest()`.
        4. Server creates the delegation token with `server.createDelegationToken(...)`.
        5. Client sets the token with `client.updateDelegation(...)`.
        6. Client uses the delegation token to call chat completions and reads `response.choices[0].message.content`.

        ```typescript reference showGithubLink
        https://github.com/NillionNetwork/blind-module-examples/blob/main/nilai/secretllm_nextjs_nucs/app/api/chat-delegation/route.ts
        ```
    </TabItem>
    <TabItem value="python" label="Python">
        ```python reference showGithubLink
        https://github.com/NillionNetwork/nilai-py/blob/main/examples/1-delegation_token_mode.py
        ```
    </TabItem>
    <TabItem value="typescript" label="Typescript">
        ```typescript reference showGithubLink
        https://github.com/NillionNetwork/nilai-ts/blob/main/examples/1-delegation-token.ts
        ```
    </TabItem>
</Tabs>
