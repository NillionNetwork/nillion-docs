import CTABanner from '@site/src/components/CTABanner';
import ThumbsUpDown from '@site/src/components/ThumbsUpDown';

# Quickstart

With nilAI, it is possible to run AI models within a trusted execution environment (TEE). Once you have nilAI API access, you can start using LLMs on nilAI with any OpenAI-compatible library. This quickstart uses the latest `secretllm_nextjs` example to build a private chat app with API key authentication.

## Getting Started

In this quickstart, you will run a private AI chat application via Next.js.

## Prerequisites

- Node.js 18 or newer
- A package manager (pnpm recommended)
- A nilAI API key

## Project Setup

1. Clone the examples repo and move into the Next.js example:

```bash
gh repo clone NillionNetwork/blind-module-examples
cd blind-module-examples/nilai/secretllm_nextjs
```

2. Create your `.env` file:

```bash
cp .env.example .env
```

Now we need to set `NILLION_API_KEY` using the API KEY from the nilAI Developer Dashboard.

<CTABanner
  question='Get your nilAI API keys here'
  instruction='Visit the nilAI Developer Dashboard and create a API key'
  buttonText='Get your keys'
  buttonLink='https://nilai.nillion.com'
  event='quickstart-sllm-cta-banner'
  external={true}
/>

3. Get your API Key from above and then add your nilAI API key:

```bash
NILLION_API_KEY=YOUR_API_KEY_HERE
```

4. Install dependencies and run the app:

```bash
pnpm i
pnpm run dev
```

5. Open `http://localhost:3000` and send a chat message.

## Usage

![secretLLM quickstart](/img/secretllm-quickstart.png)

The Next.js route in this example:

1. Reads `NILLION_API_KEY` from your `.env` file.
2. Sends your messages to `https://api.nilai.nillion.network/v1/chat/completions`.
3. Uses model `openai/gpt-oss-20b`.
4. Returns the response text (and signature when present).

```typescript reference showGithubLink
https://github.com/NillionNetwork/blind-module-examples/blob/main/nilai/secretllm_nextjs/app/api/chat/route.ts
```

## Verification

After submitting a prompt in the UI:

- You should see an assistant reply in the chat panel.
- If configuration is wrong, the chat UI shows the API error message.

### Customization

You can also customize the types of models you want to use. Currently available models are listed [here.](./overview#available-models)

## What you've done

ðŸŽ‰ **Congratulations!** You just built and interacted with a privacyâ€‘preserving LLM application:

1. **You (Builder)** get access to the secretLLM SDK.
2. **You (User)** can provide a prompt to the LLM.
3. **The LLM** understands your prompt and returns an answer via direct API key access.

This demonstrates a core principle of private AI: you can create endless private AI applications via Nillion.

<ThumbsUpDown
  thumbsUpEvent='quickstart_secretllm_completed_yes'
  thumbsDownEvent='quickstart_secretllm_completed_no'
/>
