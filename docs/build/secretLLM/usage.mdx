import Tabs from '@theme/Tabs';
import TabItem from '@theme/TabItem';

# Usage

Once you have a [NilAI API key and node base URL](build/secretLLM/access.md), you can start using SecretLLM with any OpenAI-compatible library.

## Getting Started with SecretLLM

Getting started with SecretLLM is straightforward:

1. Query the [`/v1/models`](/api/nilai/get-models-v-1-models-get) endpoint to check available models
2. Select a model and use it with the [`/v1/chat/completions`](/api/nilai/chat-completion-v-1-chat-completions-post) endpoint

Since SecretLLM is OpenAI-compatible, you can use any OpenAI library. Here's an example for querying the `Llama-3.1-8B` model:

<Tabs>

<TabItem value="python" label="Python" default>
```python
from openai import OpenAI

# Initialize the OpenAI client

client = OpenAI( # Replace <node> with the specific node identifier
    base_url="https://nilai-<node>.nillion.network/v1/",
    api_key="YOUR_API_KEY"
)

# Send a chat completion request

response = client.chat.completions.create(
    model="meta-llama/Llama-3.1-8B-Instruct",
    messages=[
        {
            "role": "system",
            "content": "You are a helpful assistant."
        },
        {
            "role": "user",
            "content": "What is your name?"
        }
    ],
    stream=False
)

````
</TabItem>

<TabItem value="node" label="Node" default>
```javascript
const OpenAI = require('openai');

// Initialize the OpenAI client
const client = new OpenAI({
  baseURL: 'https://nilai-<node>.nillion.network/v1/',
  apiKey: 'YOUR_API_KEY'
});

// Send a chat completion request
async function getChatCompletion() {
  try {
    const response = await client.chat.completions.create({
      model: 'meta-llama/Llama-3.1-8B-Instruct',
      messages: [
        {
          role: 'system',
          content: 'You are a helpful assistant.'
        },
        {
          role: 'user',
          content: 'What is your name?'
        }
      ],
      stream: false
    });

    console.log(response);
  } catch (error) {
    console.error('Error:', error);
  }
}

// Call the function
getChatCompletion();
````

</TabItem>

</Tabs>


## SecretLLM Endpoints

SecretLLM provides the following endpoints:

| Name | Endpoint | Description |
|----------|----------|-------------|
| Chat | [`/v1/chat/completions`](/api/nilai/chat-completion-v-1-chat-completions-post) | Generate AI responses |
| Models | [`/v1/models`](/api/nilai/get-models-v-1-models-get) | List available models |
| Attestation | [`/v1/attestation/report`](/api/nilai/get-attestation-v-1-attestation-report-get) | Get cryptographic proof of environment |
| Usage | [`/v1/usage`](/api/nilai/get-usage-v-1-usage-get) | Track your token usage |
| Health | [`/v1/health`](/api/nilai/health-check-v-1-health-get) | Check service status |